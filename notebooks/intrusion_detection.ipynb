{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection System Using Machine Learning\n",
    "\n",
    "This notebook demonstrates the complete workflow for building an intrusion detection system using the NSL-KDD dataset.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading](#Data-Loading)\n",
    "2. [Data Understanding](#Data-Understanding)\n",
    "3. [Data Preprocessing](#Data-Preprocessing)\n",
    "4. [Model Training](#Model-Training)\n",
    "5. [Model Evaluation](#Model-Evaluation)\n",
    "6. [Results & Conclusion](#Results-&-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "First, let's load the NSL-KDD dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "import pickle\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define column names for NSL-KDD dataset\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
    "    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
    "    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('../data/KDDTrain+.txt', header=None, names=columns)\n",
    "test_df = pd.read_csv('../data/KDDTest+.txt', header=None, names=columns)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "print(\"\\nFirst few rows of training data:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Let's examine the target variable and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the target variable\n",
    "print(\"Unique labels in training data:\")\n",
    "print(train_df['label'].unique())\n",
    "\n",
    "# Convert to binary classification\n",
    "train_df['label'] = train_df['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "test_df['label'] = test_df['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "\n",
    "# Check class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "train_df['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xlabel('Class (0: Normal, 1: Attack)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\nNormal traffic: {train_df['label'].value_counts()[0]} ({train_df['label'].value_counts()[0]/len(train_df)*100:.2f}%)\")\n",
    "print(f\"Attack traffic: {train_df['label'].value_counts()[1]} ({train_df['label'].value_counts()[1]/len(train_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now we'll preprocess the data: encode categorical features, scale numerical features, and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets for consistent preprocessing\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "numerical_features = [col for col in columns[:-1] if col not in categorical_features]\n",
    "\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "print(\"Number of numerical features:\", len(numerical_features))\n",
    "\n",
    "# One-Hot Encode categorical features\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_categorical = encoder.fit_transform(combined_df[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Combine with numerical features\n",
    "X = pd.concat([combined_df[numerical_features], encoded_df], axis=1)\n",
    "y = combined_df['label']\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# Split back into train and test\n",
    "train_size = len(train_df)\n",
    "X_train = X_scaled[:train_size]\n",
    "X_test = X_scaled[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_test = y[train_size:]\n",
    "\n",
    "print(f\"\\nPreprocessed data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Let's train Logistic Regression and Random Forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression training completed.\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Random Forest training completed.\")\n",
    "\n",
    "# Save models\n",
    "with open('../models/logistic_regression.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "with open('../models/random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(\"\\nModels saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now let's evaluate both models using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate a model and return metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n--- {model_name} Results ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Attack'],\n",
    "                yticklabels=['Normal', 'Attack'])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Attack']))\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Evaluate both models\n",
    "lr_metrics = evaluate_model(lr_model, X_test, y_test, \"Logistic Regression\")\n",
    "rf_metrics = evaluate_model(rf_model, X_test, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Conclusion\n",
    "\n",
    "Let's compare the models and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "accuracies = [lr_metrics[0], rf_metrics[0]]\n",
    "precisions = [lr_metrics[1], rf_metrics[1]]\n",
    "recalls = [lr_metrics[2], rf_metrics[2]]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "plt.bar(x, precisions, width, label='Precision', alpha=0.8)\n",
    "plt.bar(x + width, recalls, width, label='Recall', alpha=0.8)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Top 20 Feature Importances (Random Forest)\")\n",
    "    plt.bar(range(20), importances[indices][:20])\n",
    "    plt.xticks(range(20), [X_train.columns[i] for i in indices[:20]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "- Both Logistic Regression and Random Forest performed well on the NSL-KDD dataset\n",
    "- Random Forest generally achieved higher accuracy and recall\n",
    "- Feature importance analysis shows which network features are most indicative of attacks\n",
    "\n",
    "### Why These Models?\n",
    "- **Logistic Regression**: Interpretable, fast, provides probability scores\n",
    "- **Random Forest**: Handles complex patterns, robust to overfitting, feature selection capability\n",
    "\n",
    "### Security Context:\n",
    "- High recall is crucial to avoid missing attacks\n",
    "- Precision helps reduce false alarms in production systems\n",
    "- Both models can be deployed for real-time intrusion detection\n",
    "\n",
    "### Limitations:\n",
    "- Dataset is from 1999, may not represent modern attack patterns\n",
    "- Binary classification doesn't distinguish between attack types\n",
    "- No real-time processing demonstrated\n",
    "\n",
    "### Future Improvements:\n",
    "- Implement cross-validation for more robust evaluation\n",
    "- Add feature selection to reduce model complexity\n",
    "- Explore deep learning approaches for better accuracy\n",
    "- Integrate with packet capture tools for real-time detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
